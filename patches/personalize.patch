diff --git a/segment_anything/modeling/mask_decoder.py b/segment_anything/modeling/mask_decoder.py
index 5d2fdb0..a2f7e64 100644
--- a/segment_anything/modeling/mask_decoder.py
+++ b/segment_anything/modeling/mask_decoder.py
@@ -67,6 +67,8 @@ class MaskDecoder(nn.Module):
         self.iou_prediction_head = MLP(
             transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth
         )
+        # TODO: add custom heads, maybe?
+        # i.e. class prediction head
 
     def forward(
         self,
@@ -75,6 +77,8 @@ class MaskDecoder(nn.Module):
         sparse_prompt_embeddings: torch.Tensor,
         dense_prompt_embeddings: torch.Tensor,
         multimask_output: bool,
+        attn_sim=None,
+        target_embedding=None
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """
         Predict masks given image and prompt embeddings.
@@ -96,6 +100,8 @@ class MaskDecoder(nn.Module):
             image_pe=image_pe,
             sparse_prompt_embeddings=sparse_prompt_embeddings,
             dense_prompt_embeddings=dense_prompt_embeddings,
+            attn_sim=attn_sim,
+            target_embedding=target_embedding
         )
 
         # Select the correct mask or masks for output
@@ -115,6 +121,8 @@ class MaskDecoder(nn.Module):
         image_pe: torch.Tensor,
         sparse_prompt_embeddings: torch.Tensor,
         dense_prompt_embeddings: torch.Tensor,
+        attn_sim=None,
+        target_embedding=None
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """Predicts masks. See 'forward' for more details."""
         # Concatenate output tokens
@@ -129,7 +137,7 @@ class MaskDecoder(nn.Module):
         b, c, h, w = src.shape
 
         # Run the transformer
-        hs, src = self.transformer(src, pos_src, tokens)
+        hs, src = self.transformer(src, pos_src, tokens, attn_sim, target_embedding)
         iou_token_out = hs[:, 0, :]
         mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]
 
diff --git a/segment_anything/modeling/prompt_encoder.py b/segment_anything/modeling/prompt_encoder.py
index c3143f4..a1a5d9d 100644
--- a/segment_anything/modeling/prompt_encoder.py
+++ b/segment_anything/modeling/prompt_encoder.py
@@ -125,6 +125,7 @@ class PromptEncoder(nn.Module):
     def _get_device(self) -> torch.device:
         return self.point_embeddings[0].weight.device
 
+    # TODO: add image embedding vectors, CLIP embedding vectors
     def forward(
         self,
         points: Optional[Tuple[torch.Tensor, torch.Tensor]],
diff --git a/segment_anything/modeling/transformer.py b/segment_anything/modeling/transformer.py
index 28fafea..d1fb0e3 100644
--- a/segment_anything/modeling/transformer.py
+++ b/segment_anything/modeling/transformer.py
@@ -64,6 +64,8 @@ class TwoWayTransformer(nn.Module):
         image_embedding: Tensor,
         image_pe: Tensor,
         point_embedding: Tensor,
+        attn_sim: Tensor,
+        target_embedding=None
     ) -> Tuple[Tensor, Tensor]:
         """
         Args:
@@ -94,11 +96,17 @@ class TwoWayTransformer(nn.Module):
                 keys=keys,
                 query_pe=point_embedding,
                 key_pe=image_pe,
+                attn_sim=attn_sim,
             )
 
         # Apply the final attention layer from the points to the image
         q = queries + point_embedding
         k = keys + image_pe
+
+
+        if target_embedding is not None:
+            q += target_embedding
+
         attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)
         queries = queries + attn_out
         queries = self.norm_final_attn(queries)
@@ -149,7 +157,7 @@ class TwoWayAttentionBlock(nn.Module):
         self.skip_first_layer_pe = skip_first_layer_pe
 
     def forward(
-        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor
+        self, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor, attn_sim: Tensor
     ) -> Tuple[Tensor, Tensor]:
         # Self attention block
         if self.skip_first_layer_pe:
@@ -163,7 +171,7 @@ class TwoWayAttentionBlock(nn.Module):
         # Cross attention block, tokens attending to image embedding
         q = queries + query_pe
         k = keys + key_pe
-        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)
+        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys, attn_sim=attn_sim)
         queries = queries + attn_out
         queries = self.norm2(queries)
 
@@ -215,7 +223,7 @@ class Attention(nn.Module):
         x = x.transpose(1, 2)
         return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C
 
-    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:
+    def forward(self, q: Tensor, k: Tensor, v: Tensor, attn_sim: Tensor=None) -> Tensor:
         # Input projections
         q = self.q_proj(q)
         k = self.k_proj(k)
@@ -232,6 +240,10 @@ class Attention(nn.Module):
         attn = attn / math.sqrt(c_per_head)
         attn = torch.softmax(attn, dim=-1)
 
+        if attn_sim is not None:
+            attn = attn + attn_sim
+            attn = torch.softmax(attn, dim=-1)
+
         # Get output
         out = attn @ v
         out = self._recombine_heads(out)
diff --git a/segment_anything/predictor.py b/segment_anything/predictor.py
index 8a6e6d8..99a17e5 100644
--- a/segment_anything/predictor.py
+++ b/segment_anything/predictor.py
@@ -58,6 +58,15 @@ class SamPredictor:
         input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
 
         self.set_torch_image(input_image_torch, image.shape[:2])
+    
+    def preprocess_mask(self, mask: np.ndarray)->torch.Tensor:
+        # Transform the mask to the form expected by the model
+        input_mask = self.transform.apply_image(mask)
+        input_mask_torch = torch.as_tensor(input_mask, device=self.device)
+        input_mask_torch = input_mask_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
+
+        input_mask = self.model.preprocess(input_mask_torch)  # pad to 1024
+        return input_mask
 
     @torch.no_grad()
     def set_torch_image(
@@ -97,6 +106,9 @@ class SamPredictor:
         mask_input: Optional[np.ndarray] = None,
         multimask_output: bool = True,
         return_logits: bool = False,
+        attn_sim = None,
+        target_embedding = None,
+        high_res = False,
     ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
         """
         Predict masks for the given input prompts, using the currently set image.
@@ -120,6 +132,10 @@ class SamPredictor:
             input prompts, multimask_output=False can give better results.
           return_logits (bool): If true, returns un-thresholded masks logits
             instead of a binary mask.
+          attn_sim (torch.Tensor): A mask of embeddings similarity scores, used in attention.
+          target_embedding (torch.Tensor): A target embedding, used in attention.
+          high_res (bool): If true, returns high resolution masks.
+
 
         Returns:
           (np.ndarray): The output masks in CxHxW format, where C is the
@@ -151,19 +167,28 @@ class SamPredictor:
             mask_input_torch = torch.as_tensor(mask_input, dtype=torch.float, device=self.device)
             mask_input_torch = mask_input_torch[None, :, :, :]
 
-        masks, iou_predictions, low_res_masks = self.predict_torch(
+        masks, iou_predictions, low_res_masks, *rest = self.predict_torch(
             coords_torch,
             labels_torch,
             box_torch,
             mask_input_torch,
             multimask_output,
             return_logits=return_logits,
+            attn_sim=attn_sim,
+            target_embedding=target_embedding,
+            high_res=high_res,
         )
 
         masks_np = masks[0].detach().cpu().numpy()
         iou_predictions_np = iou_predictions[0].detach().cpu().numpy()
         low_res_masks_np = low_res_masks[0].detach().cpu().numpy()
-        return masks_np, iou_predictions_np, low_res_masks_np
+
+        if high_res:
+          high_res_masks = rest[0]
+          high_res_masks = high_res_masks[0]
+          return masks_np, iou_predictions_np, low_res_masks_np, high_res_masks
+        else:
+          return masks_np, iou_predictions_np, low_res_masks_np
 
     @torch.no_grad()
     def predict_torch(
@@ -174,6 +199,9 @@ class SamPredictor:
         mask_input: Optional[torch.Tensor] = None,
         multimask_output: bool = True,
         return_logits: bool = False,
+        attn_sim:torch.Tensor = None,
+        target_embedding:torch.Tensor = None,
+        high_res:bool = False,
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
         """
         Predict masks for the given input prompts, using the currently set image.
@@ -200,6 +228,9 @@ class SamPredictor:
             input prompts, multimask_output=False can give better results.
           return_logits (bool): If true, returns un-thresholded masks logits
             instead of a binary mask.
+          attn_sim (torch.Tensor): A mask of embeddings similarity scores, used in attention.
+          target_embedding (torch.Tensor): A target embedding, used in attention.
+          high_res (bool): If true, returns high resolution masks.
 
         Returns:
           (torch.Tensor): The output masks in BxCxHxW format, where C is the
@@ -232,15 +263,20 @@ class SamPredictor:
             sparse_prompt_embeddings=sparse_embeddings,
             dense_prompt_embeddings=dense_embeddings,
             multimask_output=multimask_output,
+            attn_sim=attn_sim,
+            target_embedding=target_embedding
         )
 
         # Upscale the masks to the original image resolution
-        masks = self.model.postprocess_masks(low_res_masks, self.input_size, self.original_size)
+        high_res_masks = self.model.postprocess_masks(low_res_masks, self.input_size, self.original_size)
 
         if not return_logits:
-            masks = masks > self.model.mask_threshold
+            masks = high_res_masks > self.model.mask_threshold  # 0.0
 
-        return masks, iou_predictions, low_res_masks
+        if high_res:
+          return masks, iou_predictions, low_res_masks, high_res_masks
+        else:
+          return masks, iou_predictions, low_res_masks
 
     def get_image_embedding(self) -> torch.Tensor:
         """
