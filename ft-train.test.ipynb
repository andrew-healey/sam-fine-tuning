{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "num_classes = 0\n",
    "# Use a Ladder CNN in addition to the image encoder?\n",
    "use_cnn = False\n",
    "# Run the resizing operations after the CNN? (might preserve fine pixel-level details better)\n",
    "resize_before_cnn = True\n",
    "\n",
    "# Use LoRA on the mask decoder? (if False, trains the whole decoder)\n",
    "mask_lora = True\n",
    "mask_r = 8\n",
    "\n",
    "# use LoRa on the image encoder transformer?\n",
    "vit_lora = False\n",
    "vit_r = 8\n",
    "\n",
    "# train the image encoder's patch embedding CNN? Highly recommended.\n",
    "vit_patch_embed = True\n",
    "\n",
    "\"\"\"\n",
    "TRAINING\n",
    "\"\"\"\n",
    "run_grad = True\n",
    "\n",
    "focal_scale = 20\n",
    "mse_scale = 1\n",
    "dice_scale = 1\n",
    "cls_scale = 1\n",
    "\n",
    "\"\"\"\n",
    "DATA\n",
    "\"\"\"\n",
    "use_valid = True\n",
    "cls_ids = None # any\n",
    "train_size = None # images\n",
    "valid_size = 20 # images\n",
    "\n",
    "use_masks = True\n",
    "grow_masks = False\n",
    "growth_radius = 15\n",
    "\n",
    "# train on semantic segmentation or point/box-to-mask?\n",
    "tasks = [\"sem_seg\",\"point\",\"box\",\"cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.relpath(\"./segment-anything\"))\n",
    "import segment_anything\n",
    "\n",
    "%cd datasets/\n",
    "\n",
    "from fine_tune.configs.midas_box import *\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "from fine_tune.datasets import extract_classes_from_dataset,shrink_dataset_to_size\n",
    "\n",
    "train_dataset = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/train\",\n",
    "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
    "    force_masks=use_masks\n",
    ")\n",
    "\n",
    "if train_size is not None:\n",
    "    train_dataset = shrink_dataset_to_size(train_dataset,train_size)\n",
    "\n",
    "if cls_ids is not None:\n",
    "    print(\"Selecting classes\",[train_dataset.classes[i] for i in cls_ids])\n",
    "    train_dataset = extract_classes_from_dataset(train_dataset,cls_ids)\n",
    "\n",
    "if use_valid:\n",
    "    valid_dataset = sv.DetectionDataset.from_coco(\n",
    "        images_directory_path=f\"{dataset.location}/valid\",\n",
    "        annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
    "        force_masks=use_masks\n",
    "    )\n",
    "else:\n",
    "    valid_dataset = train_dataset\n",
    "\n",
    "if valid_size is not None:\n",
    "    valid_dataset = shrink_dataset_to_size(valid_dataset,valid_size)\n",
    "\n",
    "if cls_ids is not None:\n",
    "    valid_dataset = extract_classes_from_dataset(valid_dataset,cls_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_valid:\n",
    "    valid_names = set(k.split(\".rf\")[0] for k in valid_dataset.images.keys())\n",
    "    train_names = set(k.split(\".rf\")[0] for k in train_dataset.images.keys())\n",
    "\n",
    "    # Check that there's no training/valid pollution\n",
    "    assert len(valid_names.intersection(train_names)) == 0,\"There is overlap between the training and validation sets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from persam.load import load_predictor\n",
    "from fine_tune.samed import LoRA_Mask_Decoder,LoRA_Tiny_Image_Encoder\n",
    "from fine_tune.ladder_cnn import CNN_SAM\n",
    "\n",
    "predictor = load_predictor(\"vit_t\",num_classes=num_classes)\n",
    "\n",
    "if mask_lora:\n",
    "    lora_mask_decoder = LoRA_Mask_Decoder(predictor.model.mask_decoder,r=mask_r)\n",
    "    mask_decoder = lora_mask_decoder.mask_decoder.cuda()\n",
    "else:\n",
    "    mask_decoder = predictor.model.mask_decoder\n",
    "\n",
    "if vit_lora:\n",
    "    lora_image_encoder = LoRA_Tiny_Image_Encoder(predictor.model.image_encoder,r=vit_r).cuda()\n",
    "    image_encoder = lora_image_encoder.image_encoder\n",
    "else:\n",
    "    image_encoder = predictor.model.image_encoder\n",
    "\n",
    "if use_cnn:\n",
    "    cnn_sam = CNN_SAM(resize_before_cnn=resize_before_cnn).to(\"cuda\")\n",
    "else:\n",
    "    cnn_sam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune.common import grow_dataset_masks\n",
    "\n",
    "if grow_masks:\n",
    "    grow_dataset_masks(train_dataset,growth_radius=growth_radius)\n",
    "    grow_dataset_masks(valid_dataset,growth_radius=growth_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune.common import SamBoxDataset, SamPointDataset, SamNextMaskDataset, RandomPointDataset, SamSemSegDataset, SamComboDataset, get_max_iou_masks\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "dataset_makers = {\n",
    "    \"sem_seg\": lambda ds: SamSemSegDataset(ds,predictor,device),\n",
    "    \"box\": lambda ds: SamBoxDataset(ds,predictor,device),\n",
    "    \"point\": lambda ds: SamPointDataset(ds,predictor,device,points_per_mask=1),\n",
    "    \"cls\": lambda ds: SamMidasBinaryClassification(ds,predictor,device),\n",
    "}\n",
    "\n",
    "def prep_datasets(ds):\n",
    "    datasets = []\n",
    "    for task in tasks:\n",
    "        datasets.append(dataset_makers[task](ds))\n",
    "    return SamComboDataset(datasets)\n",
    "\n",
    "curr_dataset = prep_datasets(train_dataset)\n",
    "valid_curr_dataset = prep_datasets(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "initial_lr = 2e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "warmup_steps = 25000\n",
    "total_steps = 150_000\n",
    "batch_size = 5 # perform gradient accumulation for this--they do 256 images per batch.\n",
    "log_period = 200 # a few batches\n",
    "eval_period = 500\n",
    "wandb_log_period = 20\n",
    "\n",
    "lr_decay_steps = [2/3., 0.95]\n",
    "lr_decay_steps = [int(total_steps * step) for step in lr_decay_steps]\n",
    "\n",
    "lr_decay_factor = 0.1\n",
    "\n",
    "combined_params = list(predictor.model.mask_decoder.parameters())\n",
    "if use_cnn: combined_params += list(cnn_sam.parameters())\n",
    "if vit_patch_embed: combined_params += list(image_encoder.patch_embed.parameters())\n",
    "optimizer = optim.AdamW(combined_params, lr=initial_lr, betas=(0.9, 0.999), weight_decay=weight_decay) # criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_params = sum([np.prod(p.size()) for p in combined_params if p.requires_grad])\n",
    "print(f\"Total trainable parameters: {num_params}\")\n",
    "\n",
    "# Learning rate warmup schedule\n",
    "def warmup_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "\n",
    "# Step-wise learning rate decay schedule\n",
    "def lr_decay_lambda(current_step):\n",
    "    if current_step in lr_decay_steps:\n",
    "        return lr_decay_factor\n",
    "    return 1.0\n",
    "\n",
    "lr_decay_scheduler = LambdaLR(optimizer, lr_lambda=lr_decay_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "import cv2\n",
    "from numpy.random import permutation\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from persam.persam_f import calculate_sigmoid_focal_loss, calculate_dice_loss\n",
    "\n",
    "from fine_tune.viz import mask_to_img,clip_together_imgs, show_confusion_matrix, render_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from persam.persam_f import calculate_iou_loss, calculate_dice_loss, calculate_sigmoid_focal_loss\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# cross entropy loss with logits\n",
    "bce = nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate():\n",
    "    pred_classes = []\n",
    "    gt_classes = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_count = 0\n",
    "    for decoder_input, gt_masks, gt_cls_logits, (input_size,original_size), img, (unresized_img,resized_img) in tqdm(valid_curr_dataset):\n",
    "        with torch.no_grad():\n",
    "            new_input = {**decoder_input}\n",
    "            if use_cnn:\n",
    "                cnn_embedding = cnn_sam(unresized_img,resized_img)[0]\n",
    "                curr_embedding = decoder_input[\"image_embeddings\"]\n",
    "                new_input[\"image_embeddings\"] = curr_embedding + cnn_embedding\n",
    "\n",
    "            low_res_masks, iou_predictions, cls_pred_logits = mask_decoder(**new_input)\n",
    "\n",
    "            upscaled_masks = predictor.model.postprocess_masks(low_res_masks, input_size, original_size).to(device)\n",
    "            binary_masks = normalize(threshold(upscaled_masks + 0.1, 0.0, 0)).to(device)[0,:,:,:]\n",
    "\n",
    "            gt_binary_mask, binary_mask,*_ = get_max_iou_masks(gt_masks,binary_masks)\n",
    "\n",
    "            max_idx = torch.argmax(iou_predictions)\n",
    "            binary_mask = binary_masks[max_idx]\n",
    "            pred_iou = iou_predictions[max_idx]\n",
    "\n",
    "            pred_logits = cls_pred_logits[0,max_idx]\n",
    "            assert pred_logits.shape == (mask_decoder.num_classes,),f\"pred_logits.shape: {pred_logits.shape}\"\n",
    "            if gt_cls_logits is None:\n",
    "                cls_loss = 0\n",
    "            else:\n",
    "                cls_loss = bce(pred_logits, gt_cls_logits[0].to(device))\n",
    "\n",
    "                pred_class = torch.argmax(pred_logits)\n",
    "                gt_class = torch.argmax(gt_cls_logits[0])\n",
    "\n",
    "                pred_classes.append(pred_class.item())\n",
    "                gt_classes.append(gt_class.item())\n",
    "\n",
    "            flat_binary_mask = binary_mask.view(1,-1)\n",
    "            flat_gt_binary_mask = gt_binary_mask.view(1,-1)\n",
    "\n",
    "            focal_loss = calculate_sigmoid_focal_loss(flat_binary_mask, flat_gt_binary_mask,should_sigmoid=False)\n",
    "            dice_loss = calculate_dice_loss(flat_binary_mask, flat_gt_binary_mask, should_sigmoid=False)\n",
    "            iou_loss = calculate_iou_loss(flat_binary_mask, flat_gt_binary_mask, should_sigmoid=False)\n",
    "\n",
    "            loss = dice_scale * dice_loss + focal_scale * focal_loss + cls_scale * cls_loss\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_count += 1\n",
    "    valid_loss = running_loss/running_count\n",
    "\n",
    "    wandb.log({\n",
    "        \"valid_loss\": valid_loss,\n",
    "    })\n",
    "\n",
    "    print(f\"VALID - Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    if len(gt_classes) > 0:\n",
    "        # calculate confusion matrix\n",
    "        show_confusion_matrix(gt_classes, pred_classes, class_names=valid_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=\"sam-fine-tune\",\n",
    "    config={\n",
    "        \"learning_rate\": initial_lr,\n",
    "        \"iters\": total_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr_decay_steps\": json.dumps(lr_decay_steps),\n",
    "    })\n",
    "\n",
    "curr_iters = 0\n",
    "accumulated_loss = 0\n",
    "\n",
    "# track running avg of loss\n",
    "recent_losses = []\n",
    "recent_mses = []\n",
    "\n",
    "use_postprocess = True\n",
    "\n",
    "curr_epoch = 0\n",
    "\n",
    "# iter through dataset in random order\n",
    "while curr_iters < total_steps:\n",
    "    evaluate()\n",
    "    for i,idx in enumerate(tqdm(permutation(len(curr_dataset)))):\n",
    "        decoder_input, gt_masks, gt_cls_logits, (input_size,original_size), img, (unresized_img,resized_img) = curr_dataset[idx]\n",
    "        new_input = {**decoder_input}\n",
    "\n",
    "        curr_iters += 1\n",
    "\n",
    "        _,H,W = gt_masks.shape\n",
    "\n",
    "        if vit_lora:\n",
    "            sam_embedding = image_encoder(resized_img)[0]\n",
    "            new_input[\"image_embeddings\"] = sam_embedding\n",
    "        if use_cnn:\n",
    "            cnn_embedding = cnn_sam(unresized_img, resized_img).squeeze(0)\n",
    "\n",
    "            curr_embedding = new_input[\"image_embeddings\"]\n",
    "\n",
    "            new_input[\"image_embeddings\"] = curr_embedding + cnn_embedding\n",
    "        \n",
    "\n",
    "        low_res_masks, iou_predictions, cls_pred_logits = mask_decoder(**new_input)\n",
    "\n",
    "        high_res_masks = F.interpolate(\n",
    "                low_res_masks,\n",
    "                (H, W),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        high_res_masks = high_res_masks.squeeze(0)\n",
    "        assert len(high_res_masks.shape) == 3,f\"Shape is {high_res_masks.shape}\" # 1 or 3, H, W\n",
    "\n",
    "        # upscaled_masks = F.interpolate(low_res_masks, size=img.shape[:2], mode=\"bilinear\", align_corners=False)\n",
    "        upscaled_masks = predictor.model.postprocess_masks(low_res_masks, input_size, original_size).to(device)\n",
    "        binary_masks = normalize(threshold(upscaled_masks, 0.0, 0)).to(device)[0,:,:,:]\n",
    "\n",
    "        gt_binary_mask, binary_mask, iou, pred_idx = get_max_iou_masks(gt_masks,binary_masks)\n",
    "        pred_iou = iou_predictions[0,pred_idx]\n",
    "\n",
    "        pred_mask = upscaled_masks[0,pred_idx] if use_postprocess else high_res_masks[pred_idx]\n",
    "\n",
    "        pred_logits = cls_pred_logits[0,pred_idx]\n",
    "        assert pred_logits.shape == (mask_decoder.num_classes,),f\"pred_logits.shape: {pred_logits.shape}\"\n",
    "\n",
    "        if gt_cls_logits is None:\n",
    "            cls_loss = 0\n",
    "        else:\n",
    "            cls_loss = bce(pred_logits, gt_cls_logits[0].to(device))\n",
    "\n",
    "        assert gt_binary_mask.shape == binary_mask.shape,f\"Mismatched shapes {gt_binary_mask.shape}, {binary_mask.shape}\"\n",
    "        h,w = gt_binary_mask.shape\n",
    "\n",
    "        flat_pred_mask = pred_mask.view(1,-1)\n",
    "        flat_gt_binary_mask = gt_binary_mask.view(1,-1)\n",
    "\n",
    "        focal_loss = calculate_sigmoid_focal_loss(flat_pred_mask, flat_gt_binary_mask)\n",
    "        dice_loss = calculate_dice_loss(flat_pred_mask, flat_gt_binary_mask)\n",
    "        mse_loss = mse(pred_iou, iou)\n",
    "\n",
    "        loss = dice_scale * dice_loss + mse_scale * mse_loss + focal_scale * focal_loss + cls_scale * cls_loss\n",
    "\n",
    "        recent_losses += [loss.item()]\n",
    "        recent_losses = recent_losses[-log_period:]\n",
    "\n",
    "        if i % wandb_log_period == 0:\n",
    "            wandb.log({\n",
    "                \"loss\": loss.cpu().item(),\n",
    "                \"focal_loss\": focal_loss.cpu().item(),\n",
    "                \"dice_loss\": dice_loss.cpu().item(),\n",
    "                \"mse_loss\": mse_loss.cpu().item(),\n",
    "                \"cls_loss\": cls_loss.cpu().item(),\n",
    "            })\n",
    "\n",
    "        if i % log_period == 0:\n",
    "            print(f\"Loss: {sum(recent_losses)/len(recent_losses)}\")\n",
    "\n",
    "        if not run_grad: continue\n",
    "        accumulated_loss += loss\n",
    "        if curr_iters % batch_size == 0:\n",
    "            optimizer.zero_grad()\n",
    "            accumulated_loss /= batch_size\n",
    "            accumulated_loss.backward()\n",
    "            optimizer.step()\n",
    "            accumulated_loss = 0\n",
    "        \n",
    "        warmup_scheduler.step(curr_iters)\n",
    "        lr_decay_scheduler.step(curr_iters)\n",
    "    \n",
    "    curr_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_together_imgs(mask_to_img(pred_mask > 0,img),mask_to_img(gt_binary_mask,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "%cd runs/\n",
    "\n",
    "run_cfgs = glob('cfg_*.json')\n",
    "highest_run = max([int(cfg.split('_')[1].split('.')[0]) for cfg in run_cfgs])\n",
    "run_num = highest_run + 1\n",
    "print(f\"Run number: {run_num}\")\n",
    "\n",
    "dataset_path = dataset.location\n",
    "import os\n",
    "dataset_name = os.path.basename(dataset_path)\n",
    "\n",
    "cfg = {\n",
    "    \"dataset_name\": dataset_name,\n",
    "    \"use_cnn\": use_cnn,\n",
    "    \"resize_before_cnn\": resize_before_cnn,\n",
    "    \"grow_masks\": grow_masks,\n",
    "    \"mask_lora\": mask_lora,\n",
    "    \"mask_r\": mask_r,\n",
    "    \"vit_lora\": vit_lora,\n",
    "    \"vit_r\": vit_r,\n",
    "    \"vit_patch_embed\": vit_patch_embed,\n",
    "    \"initial_lr\": initial_lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"total_steps\": total_steps,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr_decay_steps\": lr_decay_steps,\n",
    "    \"lr_decay_factor\": lr_decay_factor,\n",
    "    \"run_name\":run.name,\n",
    "    \"run_url\":run.url,\n",
    "    \"run_grad\": run_grad,\n",
    "    \"use_masks\": use_masks,\n",
    "    \"dice_scale\": dice_scale,\n",
    "    \"focal_scale\": focal_scale,\n",
    "    \"cls_scale\": cls_scale,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"tasks\": tasks,\n",
    "    \"use_valid\": use_valid,\n",
    "}\n",
    "\n",
    "with open(f'cfg_{run_num}.json', 'w') as f:\n",
    "    json.dump(cfg, f)\n",
    "\n",
    "torch.save(mask_decoder.state_dict(), f'mask_decoder_{run_num}.pt')\n",
    "if use_cnn:\n",
    "    torch.save(cnn_sam.state_dict(), f'ladder_{run_num}.pt')\n",
    "if mask_lora:\n",
    "    lora_mask_decoder.save_lora_parameters(f\"mask_lora_{run_num}.pt\")\n",
    "if vit_lora:\n",
    "    lora_image_encoder.save_lora_parameters(f\"vit_lora_{run_num}.pt\")\n",
    "if vit_patch_embed:\n",
    "    torch.save(image_encoder.patch_embed.state_dict(), f\"vit_patch_embed_{run_num}.pt\")\n",
    "if num_classes > 0:\n",
    "    torch.save(mask_decoder.class_tokens.state_dict(), f\"class_tokens_{run_num}.pt\")\n",
    "    torch.save(mask_decoder.class_prediction_head.state_dict(), f\"class_prediction_head_{run_num}.pt\")\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "idx = randint(0,len(valid_curr_dataset)-1)\n",
    "\n",
    "decoder_input, gt_masks, gt_cls_logits, (input_size,original_size), img, (unresized_img,resized_img) = valid_curr_dataset[idx]\n",
    "new_input = {**decoder_input}\n",
    "print(\"one mask\" if len(gt_masks) == 1 else \"multiple masks\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    if use_cnn:\n",
    "        cnn_embedding = cnn_sam(unresized_img,resized_img)[0]\n",
    "        curr_embedding = decoder_input[\"image_embeddings\"]\n",
    "        new_input[\"image_embeddings\"] = curr_embedding + cnn_embedding\n",
    "    low_res_masks, iou_predictions, cls_pred_logits = mask_decoder(**new_input)\n",
    "\n",
    "# upscaled_masks = F.interpolate(low_res_masks, size=img.shape[:2], mode=\"bilinear\", align_corners=False)\n",
    "upscaled_masks = predictor.model.postprocess_masks(low_res_masks, input_size, original_size).to(device)\n",
    "binary_masks = normalize(threshold(upscaled_masks + 0.1, 0.0, 0)).to(device)[0,:,:,:]\n",
    "\n",
    "max_idx = torch.argmax(iou_predictions).item()\n",
    "iou = iou_predictions[max_idx].item()\n",
    "\n",
    "gt_binary_mask, binary_mask,*_ = get_max_iou_masks(gt_masks,binary_masks)\n",
    "binary_mask = binary_masks[max_idx]\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from persam.persam_f import calculate_iou_loss, calculate_dice_loss, calculate_sigmoid_focal_loss\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    pred_logits = cls_pred_logits[0,max_idx]\n",
    "    assert pred_logits.shape == (mask_decoder.num_classes,),f\"pred_logits.shape: {pred_logits.shape}\"\n",
    "    if gt_cls_logits is None:\n",
    "        cls_loss = 0\n",
    "    else:\n",
    "        cls_loss = bce(pred_logits, gt_cls_logits[0].to(device))\n",
    "\n",
    "    flat_binary_mask = binary_mask.view(1,-1)\n",
    "    flat_gt_binary_mask = gt_binary_mask.view(1,-1)\n",
    "\n",
    "    focal_loss = calculate_sigmoid_focal_loss(flat_binary_mask, flat_gt_binary_mask,should_sigmoid=False)\n",
    "    dice_loss = calculate_dice_loss(flat_binary_mask, flat_gt_binary_mask, should_sigmoid=False)\n",
    "    iou_loss = calculate_iou_loss(flat_binary_mask, flat_gt_binary_mask, should_sigmoid=False)\n",
    "    loss = focal_loss + dice_loss\n",
    "    print(f\"Loss: {loss:.4f}, Focal Loss: {focal_loss:.4f}, Dice Loss: {dice_loss:.4f}, IoU Loss: {iou_loss:.4f}\")\n",
    "\n",
    "print(\"Predicted IoU:\",iou)\n",
    "print(\"Predicted class:\",torch.argmax(pred_logits).item(),\"GT class:\",torch.argmax(gt_cls_logits[0]).item())\n",
    "\n",
    "clip_together_imgs(mask_to_img(binary_mask,img),mask_to_img(gt_binary_mask,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled_masks = F.interpolate(low_res_masks, size=img.shape[:2], mode=\"bilinear\", align_corners=False)\n",
    "# upscaled_masks = predictor.model.postprocess_masks(low_res_masks, input_size, original_size).to(device)\n",
    "\n",
    "Image.fromarray(upscaled_masks.cpu().numpy()[0,0] * 255).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale low_res_masks to 1024x1024\n",
    "high_res_masks = F.interpolate(low_res_masks, size=img.shape[:2], mode=\"bilinear\", align_corners=False).detach().cpu().numpy()[0,0]\n",
    "\n",
    "thresholded = (high_res_masks > 0.5).astype(np.uint8)\n",
    "Image.fromarray(img * thresholded[...,None]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "name,prompt = choice(cls_dataset.prompts)\n",
    "print(prompt)\n",
    "render_prompt(name,prompt,train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "from segment_anything import automatic_mask_generator\n",
    "from persam.load import load_predictor\n",
    "import cv2\n",
    "\n",
    "model = load_predictor(\"vit_h\").model.cuda()\n",
    "# model.mask_decoder = mask_decoder\n",
    "\n",
    "img = cv2.imread(\"demo-img.png\")#list(train_dataset.images.values())[0]\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# raise NotImplementedError(\"TODO: fix this\")\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(model,stability_score_thresh=0.1)\n",
    "print(f\"Model threshold: {model.mask_threshold}, offset: {mask_generator.stability_score_offset}\")\n",
    "\n",
    "sam_result = mask_generator.generate(img)\n",
    "raw_masks = automatic_mask_generator.latest_masks\n",
    "\n",
    "if len(sam_result) == 0:\n",
    "    print(\"No detections found\")\n",
    "    raise NotImplementedError(\"TODO: fix this\")\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "\n",
    "annotator = sv.MaskAnnotator()\n",
    "# paste each image into the new image\n",
    "# detections = pred_dataset.annotations[img_name]\n",
    "tmp_img = annotator.annotate(scene=img,detections=detections)\n",
    "tmp_img = Image.fromarray(tmp_img)\n",
    "\n",
    "tmp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "print(raw_masks.shape,raw_masks.dtype)\n",
    "Image.fromarray(raw_masks[0].detach().cpu().numpy().astype(np.uint8) * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"weights/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune.samed import LoRA_Mask_Decoder\n",
    "\n",
    "lora_mask_decoder = LoRA_Mask_Decoder(sam.mask_decoder,r=5)\n",
    "\n",
    "lora_mask_decoder.load_lora_parameters(\"lora.pt\")\n",
    "mask_decoder = lora_mask_decoder.mask_decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread(\"demo-img.png\")#list(train_dataset.images.values())[0]\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "masks = mask_generator.generate(img)\n",
    "\n",
    "masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervision.dataset.utils import approximate_mask_with_polygons\n",
    "\n",
    "from random import choice\n",
    "rand_img_name = choice(list(train_dataset.images.keys()))\n",
    "\n",
    "rand_img_dets = train_dataset.annotations[rand_img_name]\n",
    "\n",
    "biggest_mask = rand_img_dets.mask[rand_img_dets.area.argmax()]\n",
    "\n",
    "from supervision.dataset.utils import approximate_mask_with_polygons\n",
    "def dets_to_polygonss(dets):\n",
    "    polygons = []\n",
    "    for _,det_mask,*_ in dets:\n",
    "        polygons.append(approximate_mask_with_polygons(det_mask))\n",
    "    return polygons\n",
    "\n",
    "def get_distances(point, polygonss):\n",
    "    distances = np.zeros(len(polygonss), dtype=np.float32)\n",
    "    for i,polygons in enumerate(polygonss):\n",
    "        distances[i] = min([-cv2.pointPolygonTest(polygon, point, True) for polygon in polygons])\n",
    "    \n",
    "    distances = np.maximum(distances, 0)\n",
    "    return distances\n",
    "\n",
    "def show_closest_distances(point, dets):\n",
    "    # get the distance of each det\n",
    "    polygonss = dets_to_polygonss(dets)\n",
    "    distances = get_distances(point, polygonss)\n",
    "\n",
    "    # render the masks with their respective distances\n",
    "    empty_map = np.zeros_like(dets.mask[0], dtype=np.float32)\n",
    "\n",
    "    for i,(_,det_mask,*_) in enumerate(dets):\n",
    "        empty_map[det_mask] = distances[i]\n",
    "\n",
    "    return empty_map\n",
    "\n",
    "polygons = approximate_mask_with_polygons(biggest_mask)\n",
    "\n",
    "# make a signed distance map\n",
    "empty_map = np.zeros(biggest_mask.shape, dtype=np.float32)\n",
    "\n",
    "for i in range(biggest_mask.shape[0]):\n",
    "    for j in range(biggest_mask.shape[1]):\n",
    "        redPoint = (j,i)\n",
    "        closestDist = cv2.pointPolygonTest(polygons[0], redPoint, True)\n",
    "\n",
    "        empty_map[i, j] = -closestDist\n",
    "\n",
    "empty_map = np.maximum(empty_map, 0)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "rand_point = (choice(range(biggest_mask.shape[0])), choice(range(biggest_mask.shape[1])))\n",
    "\n",
    "plt.imshow(show_closest_distances(rand_point, rand_img_dets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive mask gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "class LabellingSession:\n",
    "    def __init__(self,img:np.ndarray):\n",
    "        self.img = img\n",
    "        self.sv_dataset = sv.DetectionDataset(\n",
    "            classes=train_dataset.classes,\n",
    "            images={\n",
    "                \"test\":img\n",
    "            },\n",
    "            annotations={\n",
    "                \"test\":sv.Detections.empty()\n",
    "            }\n",
    "        )\n",
    "        self.torch_dataset = SamNextMaskDataset(self.sv_dataset,predictor,device)\n",
    "        self.annotator = sv.MaskAnnotator()\n",
    "    \n",
    "    def update_detections(self,detections:sv.Detections):\n",
    "        self.sv_dataset.annotations[\"test\"] = detections\n",
    "\n",
    "    def img_to_tensors(self):\n",
    "        return self.torch_dataset[0]\n",
    "    \n",
    "    def show(self):\n",
    "        return Image.fromarray(self.annotator.annotate(scene=self.img,detections=self.sv_dataset.annotations[\"test\"]))\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_next_mask(self):\n",
    "        decoder_input, _, (input_size,original_size), img, (unresized_img,resized_img) = self.img_to_tensors()\n",
    "        H,W,_ = img.shape\n",
    "\n",
    "        new_input = {**decoder_input}\n",
    "\n",
    "        if vit_lora:\n",
    "            sam_embedding = image_encoder(resized_img)[0]\n",
    "            new_input[\"image_embeddings\"] = sam_embedding\n",
    "\n",
    "        if use_cnn:\n",
    "            cnn_embedding = cnn_sam(unresized_img,resized_img)[0]\n",
    "\n",
    "            curr_embedding = new_input[\"image_embeddings\"]\n",
    "\n",
    "            new_input[\"image_embeddings\"] = curr_embedding + cnn_embedding\n",
    "\n",
    "        low_res_masks, iou_predictions = mask_decoder(**new_input)\n",
    "\n",
    "        high_res_masks = F.interpolate(\n",
    "                low_res_masks,\n",
    "                (H, W),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        high_res_masks = high_res_masks.squeeze(0)\n",
    "        assert len(high_res_masks.shape) == 3,f\"Shape is {high_res_masks.shape}\" # 1 or 3, H, W\n",
    "\n",
    "        assert len(iou_predictions.shape) == 1, f\"IoU predictions shape is {iou_predictions.shape}\"\n",
    "\n",
    "        pred_idx = torch.argmax(iou_predictions)\n",
    "        pred_mask = high_res_masks[pred_idx] > 0\n",
    "\n",
    "        # new detection\n",
    "        masks = pred_mask[None, :, :].cpu().numpy()\n",
    "        class_id = 0\n",
    "        new_dets = sv.Detections(\n",
    "            xyxy=sv.Detections.mask_to_xyxy(masks),\n",
    "            mask=masks,\n",
    "            class_id=np.array([class_id]),\n",
    "        )\n",
    "\n",
    "        self.update_detections(new_dets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_img = cv2.imread(\"test.jpg\")\n",
    "\n",
    "session = LabellingSession(demo_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_next_mask()\n",
    "\n",
    "session.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
